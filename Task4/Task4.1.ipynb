{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e87a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa1f6bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The exact derivative at x=3.0 is 14.0\n",
      "The numerical derivative is: 14.000300000063248\n",
      "Partial derivative w.r.t 'a': -3.000000000010772, Expected: -3.0\n",
      "Partial derivative w.r.t 'b': 2.0000000000042206, Expected: 2.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 1: Understanding Derivatives\n",
    "# =============================================================================\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5\n",
    "\n",
    "# Let's see how the function looks\n",
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(xs, ys)\n",
    "plt.title(\"f(x) = 3x^2 - 4x + 5\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# TASK 1.1: Find the derivative of f(x) w.r.t x at x=3.0 numerically.\n",
    "h = 0.0001\n",
    "x = 3.0\n",
    "\n",
    "# Calculate the value of the function at x\n",
    "f_x = f(x)\n",
    "\n",
    "# Calculate the value of the function at x + h\n",
    "f_x_plus_h = f(x + h)\n",
    "\n",
    "# Calculate the slope (the numerical derivative)\n",
    "slope = (f_x_plus_h - f_x) / h\n",
    "\n",
    "print(f\"The exact derivative at x=3.0 is 14.0\")\n",
    "print(f\"The numerical derivative is: {slope}\")\n",
    "\n",
    "# TASK 1.2: Find the partial derivative of d with respect to 'a' and 'b'.\n",
    "h = 0.0001\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "# Initial expression\n",
    "d1 = a*b + c\n",
    "\n",
    "# --- Derivative w.r.t 'a' ---\n",
    "a_new = a + h\n",
    "d2_wrt_a = a_new*b + c\n",
    "slope_a = (d2_wrt_a - d1) / h\n",
    "print(f\"Partial derivative w.r.t 'a': {slope_a}, Expected: {b}\")\n",
    "\n",
    "# --- Derivative w.r.t 'b' ---\n",
    "b_new = b + h\n",
    "d2_wrt_b = a*b_new + c\n",
    "slope_b = (d2_wrt_b - d1) / h\n",
    "print(f\"Partial derivative w.r.t 'b': {slope_b}, Expected: {a}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c4f80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=4.0)\n",
      "{Value(data=10.0), Value(data=-6.0)}\n",
      "+\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 2: Creating an Automatic Differentiation Engine\n",
    "# =============================================================================\n",
    "\n",
    "class Value:\n",
    "    \"\"\"\n",
    "    A class for a value that can be used in a computational graph.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        # Initialize all the attributes\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "\n",
    "# Test the implementation\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "\n",
    "print(d)        # Expected: Value(data=4.0)\n",
    "print(d._prev)  # Expected: {Value(data=10.0), Value(data=-6.0)}\n",
    "print(d._op)    # Expected: '+'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4889d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphviz not available - visualization will be skipped\n",
      "L = -8.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION CODE (PROVIDED)\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    from graphviz import Digraph\n",
    "    \n",
    "    def trace(root):\n",
    "        # builds a set of all nodes and edges in a graph\n",
    "        nodes, edges = set(), set()\n",
    "        def build(v):\n",
    "            if v not in nodes:\n",
    "                nodes.add(v)\n",
    "                for child in v._prev:\n",
    "                    edges.add((child, v))\n",
    "                    build(child)\n",
    "        build(root)\n",
    "        return nodes, edges\n",
    "\n",
    "    def draw_dot(root):\n",
    "        dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})\n",
    "        \n",
    "        nodes, edges = trace(root)\n",
    "        for n in nodes:\n",
    "            uid = str(id(n))\n",
    "            dot.node(name=uid, label=\"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "            if n._op:\n",
    "                dot.node(name=uid + n._op, label=n._op)\n",
    "                dot.edge(uid + n._op, uid)\n",
    "\n",
    "        for n1, n2 in edges:\n",
    "            dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "        return dot\n",
    "    \n",
    "    print(\"Graphviz visualization available!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Graphviz not available - visualization will be skipped\")\n",
    "    def draw_dot(root):\n",
    "        print(\"Visualization skipped - graphviz not installed\")\n",
    "        return None\n",
    "\n",
    "# Create and visualize the graph\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "\n",
    "print(f\"L = {L.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40ce5b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L.grad = 1.0\n",
      "d.grad = -2.0\n",
      "f.grad = 4.0\n",
      "c.grad = -2.0\n",
      "e.grad = -2.0\n",
      "a.grad = 6.0\n",
      "b.grad = -4.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TASK 2.3: Manual Backpropagation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "L.grad = 1.0\n",
    "\n",
    "# Calculate the gradient for d and f\n",
    "# d.grad = dL/dd = f.data (because L = d * f, so dL/dd = f)\n",
    "# f.grad = dL/df = d.data (because L = d * f, so dL/df = d)\n",
    "d.grad = f.data\n",
    "f.grad = d.data\n",
    "\n",
    "print(f\"L.grad = {L.grad}\")\n",
    "print(f\"d.grad = {d.grad}\")\n",
    "print(f\"f.grad = {f.grad}\")\n",
    "\n",
    "# Calculate the gradient for c and e\n",
    "# c.grad = d.grad * 1.0 (because d = e + c, so dd/dc = 1)\n",
    "# e.grad = d.grad * 1.0 (because d = e + c, so dd/de = 1)\n",
    "c.grad = d.grad * 1.0\n",
    "e.grad = d.grad * 1.0\n",
    "\n",
    "print(f\"c.grad = {c.grad}\")\n",
    "print(f\"e.grad = {e.grad}\")\n",
    "\n",
    "# Calculate the gradient for a and b\n",
    "# a.grad = e.grad * b.data (because e = a * b, so de/da = b)\n",
    "# b.grad = e.grad * a.data (because e = a * b, so de/db = a)\n",
    "a.grad = e.grad * b.data\n",
    "b.grad = e.grad * a.data\n",
    "\n",
    "print(f\"a.grad = {a.grad}\")\n",
    "print(f\"b.grad = {b.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8879b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 2.4: Automating Backpropagation - Complete Value Class\n",
    "# =============================================================================\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            # The gradient of the output should be propagated to the inputs\n",
    "            # The local derivative for '+' is 1.0\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            # The local derivative for '*' is the other value\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            # The local derivative of tanh(x) is 1 - tanh(x)^2\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        x = self.data\n",
    "        s = 1 / (1 + math.exp(-x))\n",
    "        out = Value(s, (self,), 'sigmoid')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += s * (1 - s) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"Exponential function\"\"\"\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self):\n",
    "        \"\"\"Natural logarithm function\"\"\"\n",
    "        x = self.data\n",
    "        out = Value(math.log(x), (self,), 'log')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (1/x) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # Create a topological sort of the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        \n",
    "        # Set the gradient of the final output node to 1.0\n",
    "        self.grad = 1.0\n",
    "        \n",
    "        # Iterate through nodes in reverse topological order\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e864b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Automated Backpropagation ===\n",
      "o.grad = 1.0\n",
      "n.grad = 0.4999999999999999\n",
      "b.grad = 0.4999999999999999\n",
      "x1w1x2w2.grad = 0.4999999999999999\n",
      "x1w1.grad = 0.4999999999999999\n",
      "x2w2.grad = 0.4999999999999999\n",
      "x1.grad = -1.4999999999999996\n",
      "x2.grad = 0.4999999999999999\n",
      "w1.grad = 0.9999999999999998\n",
      "w2.grad = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test automated backpropagation on a neuron\n",
    "print(\"\\n=== Testing Automated Backpropagation ===\")\n",
    "\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "# Run the backward pass\n",
    "o.backward()\n",
    "\n",
    "print(f\"o.grad = {o.grad}\")\n",
    "print(f\"n.grad = {n.grad}\")\n",
    "print(f\"b.grad = {b.grad}\")\n",
    "print(f\"x1w1x2w2.grad = {x1w1x2w2.grad}\")\n",
    "print(f\"x1w1.grad = {x1w1.grad}\")\n",
    "print(f\"x2w2.grad = {x2w2.grad}\")\n",
    "print(f\"x1.grad = {x1.grad}\")\n",
    "print(f\"x2.grad = {x2.grad}\")\n",
    "print(f\"w1.grad = {w1.grad}\")\n",
    "print(f\"w2.grad = {w2.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99e32398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PART 3.1 : Building a Neural Network (Module)\n",
    "# =============================================================================\n",
    "\n",
    "class Module:\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, nin, activation='tanh'):\n",
    "        # A neuron has 'nin' weights (w) and one bias (b)\n",
    "        # Initialize weights randomly between -1 and 1\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "        self.activation = activation\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        # Compute the activation of the neuron\n",
    "        # Formula: act = w[0]*x[0] + w[1]*x[1] + ... + b\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        # Apply activation function\n",
    "        if self.activation == 'tanh':\n",
    "            return act.tanh()\n",
    "        elif self.activation == 'relu':\n",
    "            return act.relu()\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return act.sigmoid()\n",
    "        else:\n",
    "            return act  # linear\n",
    "  \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fd66a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Neural Network ===\n",
      "Network output: Value(data=0.2644029320377739)\n",
      "Number of parameters: 41\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 3.2 : Building a Neural Network (Layer)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, activation='tanh'):\n",
    "        # A layer is a list of neurons\n",
    "        self.neurons = [Neuron(nin, activation) for _ in range(nout)]\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        # When you call a layer, you call each neuron with the input x\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "  \n",
    "    def parameters(self):\n",
    "        # A layer's parameters are all the parameters from all its neurons\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts, activations=None):\n",
    "        # Set default activations if not provided\n",
    "        if activations is None:\n",
    "            activations = ['tanh'] * len(nouts)\n",
    "        \n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], activations[i]) for i in range(len(nouts))]\n",
    "  \n",
    "    def __call__(self, x):\n",
    "        # Pass the input 'x' through all the layers sequentially\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "  \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "# Test the network\n",
    "print(\"\\n=== Testing Neural Network ===\")\n",
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "output = n(x)\n",
    "print(f\"Network output: {output}\")\n",
    "print(f\"Number of parameters: {len(n.parameters())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1d36a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Simple Network ===\n",
      "Epoch 0 Loss 5.3342\n",
      "Epoch 1 Loss 4.3509\n",
      "Epoch 2 Loss 4.1010\n",
      "Epoch 3 Loss 3.9798\n",
      "Epoch 4 Loss 3.9289\n",
      "Epoch 5 Loss 3.8826\n",
      "Epoch 6 Loss 3.8342\n",
      "Epoch 7 Loss 3.7784\n",
      "Epoch 8 Loss 3.7095\n",
      "Epoch 9 Loss 3.6195\n",
      "Epoch 10 Loss 3.4975\n",
      "Epoch 11 Loss 3.3302\n",
      "Epoch 12 Loss 3.1119\n",
      "Epoch 13 Loss 2.8685\n",
      "Epoch 14 Loss 2.6594\n",
      "Epoch 15 Loss 2.4689\n",
      "Epoch 16 Loss 2.1014\n",
      "Epoch 17 Loss 1.3220\n",
      "Epoch 18 Loss 0.7628\n",
      "Epoch 19 Loss 0.5111\n",
      "Final predictions: [0.7139484403554412, -0.6279582724806838, -0.8818945213980857, 0.6193499334353292]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 4: Training the Network\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Training Simple Network ===\")\n",
    "\n",
    "# Dataset\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # desired targets\n",
    "\n",
    "# Initialize the network\n",
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 20\n",
    "lr = 0.05\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Forward Pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "\n",
    "    # 2. Compute Loss (Mean Squared Error)\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "\n",
    "    # 3. Backward Pass\n",
    "    # Zero out the old gradients\n",
    "    n.zero_grad()\n",
    "    # Run the backward pass to compute new gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Update Parameters\n",
    "    for p in n.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "        \n",
    "    print(f'Epoch {epoch} Loss {loss.data:.4f}')\n",
    "\n",
    "print(\"Final predictions:\", [pred.data for pred in [n(x) for x in xs]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31022620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Digit Recognition Data ===\n",
      "Training data shape: (1437, 64)\n",
      "Test data shape: (360, 64)\n",
      "Number of parameters: 2410\n",
      "\n",
      "=== Training Digit Recognition Network ===\n",
      "Epoch 1, Average Loss: 1.0074\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m create_batches(X_train, y_train_one_hot, batch_size):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# 1. Forward Pass\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(Value, xrow)) \u001b[38;5;28;01mfor\u001b[39;00m xrow \u001b[38;5;129;01min\u001b[39;00m X_batch]\n\u001b[1;32m---> 60\u001b[0m     ypred_batch \u001b[38;5;241m=\u001b[39m [\u001b[43mneural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# 2. Compute Loss for the batch\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Convert true labels to Value objects\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     y_batch_values \u001b[38;5;241m=\u001b[39m [[Value(y_val) \u001b[38;5;28;01mfor\u001b[39;00m y_val \u001b[38;5;129;01min\u001b[39;00m y_row] \u001b[38;5;28;01mfor\u001b[39;00m y_row \u001b[38;5;129;01min\u001b[39;00m y_batch]\n",
      "Cell \u001b[1;32mIn[17], line 32\u001b[0m, in \u001b[0;36mMLP.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Pass the input 'x' through all the layers sequentially\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 32\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# When you call a layer, you call each neuron with the input x\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     outs \u001b[38;5;241m=\u001b[39m [\u001b[43mn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneurons]\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m outs\n",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m, in \u001b[0;36mNeuron.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Compute the activation of the neuron\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Formula: act = w[0]*x[0] + w[1]*x[1] + ... + b\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Apply activation function\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Compute the activation of the neuron\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Formula: act = w[0]*x[0] + w[1]*x[1] + ... + b\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m((\u001b[43mwi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxi\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m wi, xi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw, x)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Apply activation function\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m, in \u001b[0;36mValue.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m     30\u001b[0m     other \u001b[38;5;241m=\u001b[39m other \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Value) \u001b[38;5;28;01melse\u001b[39;00m Value(other)\n\u001b[1;32m---> 31\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_backward\u001b[39m():\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# The local derivative for '*' is the other value\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m out\u001b[38;5;241m.\u001b[39mgrad\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36mValue.__init__\u001b[1;34m(self, data, _children, _op, label)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mValue\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, _children\u001b[38;5;241m=\u001b[39m(), _op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 5: Handwritten Digit Recognition\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== Loading Digit Recognition Data ===\")\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))  # Flatten images to 64-element vectors\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train / 16.0\n",
    "X_test = X_test / 16.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((len(y), num_classes))\n",
    "    one_hot[np.arange(len(y)), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "num_classes = 10\n",
    "y_train_one_hot = one_hot_encode(y_train, num_classes)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Network architecture: 64 inputs -> 32 hidden -> 10 outputs\n",
    "neural_network = MLP(64, [32, 10], activations=['relu', 'sigmoid'])\n",
    "print(f\"Number of parameters: {len(neural_network.parameters())}\")\n",
    "\n",
    "# Helper function for batching\n",
    "def create_batches(X, y, batch_size):\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(X))\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "print(\"\\n=== Training Digit Recognition Network ===\")\n",
    "\n",
    "lr = 1.0\n",
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for X_batch, y_batch in create_batches(X_train, y_train_one_hot, batch_size):\n",
    "        # 1. Forward Pass\n",
    "        inputs = [list(map(Value, xrow)) for xrow in X_batch]\n",
    "        ypred_batch = [neural_network(x) for x in inputs]\n",
    "\n",
    "        # 2. Compute Loss for the batch\n",
    "        # Convert true labels to Value objects\n",
    "        y_batch_values = [[Value(y_val) for y_val in y_row] for y_row in y_batch]\n",
    "        \n",
    "        # Calculate MSE loss\n",
    "        loss = Value(0)\n",
    "        for ypred, ytrue in zip(ypred_batch, y_batch_values):\n",
    "            for yp, yt in zip(ypred, ytrue):\n",
    "                loss = loss + (yp - yt)**2\n",
    "        loss = loss * (1.0 / len(X_batch))\n",
    "\n",
    "        # 3. Backward Pass\n",
    "        neural_network.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # 4. Update Parameters\n",
    "        for p in neural_network.parameters():\n",
    "            p.data -= lr * p.grad\n",
    "        \n",
    "        epoch_loss += loss.data\n",
    "        batch_count += 1\n",
    "\n",
    "    avg_loss = epoch_loss / batch_count\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n",
    "    # Decrease learning rate over time\n",
    "    lr *= 0.9\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n=== Evaluating Model ===\")\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    correct = 0\n",
    "    total = len(X_test)\n",
    "    \n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_test]\n",
    "    \n",
    "    for i in range(total):\n",
    "        outputs = model(inputs[i])\n",
    "        predicted_class = np.argmax([p.data for p in outputs])\n",
    "        \n",
    "        if predicted_class == y_test[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(neural_network, X_test, y_test)\n",
    "\n",
    "# Show sample predictions\n",
    "def show_sample_predictions(model, X_test, y_test, n_samples=5):\n",
    "    print(f\"\\n=== Sample Predictions ===\")\n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_test[:n_samples]]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        outputs = model(inputs[i])\n",
    "        predicted_class = np.argmax([p.data for p in outputs])\n",
    "        true_class = y_test[i]\n",
    "        confidence = max([p.data for p in outputs])\n",
    "        \n",
    "        status = \"✓\" if predicted_class == true_class else \"✗\"\n",
    "        print(f\"Sample {i+1}: True={true_class}, Pred={predicted_class}, Conf={confidence:.3f} {status}\")\n",
    "\n",
    "show_sample_predictions(neural_network, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87597b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.8.0-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.23.0-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.8.0-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\maram allah\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\downloads\\applicationssetup\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in d:\\downloads\\applicationssetup\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maram allah\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\downloads\\applicationssetup\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maram allah\\appdata\\roaming\\python\\python312\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\maram allah\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\maram allah\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\downloads\\applicationssetup\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maram allah\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.8.0-cp312-cp312-win_amd64.whl (241.3 MB)\n",
      "   ---------------------------------------- 0.0/241.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/241.3 MB 4.2 MB/s eta 0:00:57\n",
      "   ---------------------------------------- 1.3/241.3 MB 4.0 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 2.1/241.3 MB 3.9 MB/s eta 0:01:02\n",
      "   ---------------------------------------- 2.9/241.3 MB 4.0 MB/s eta 0:01:00\n",
      "    --------------------------------------- 3.7/241.3 MB 4.0 MB/s eta 0:01:00\n",
      "    --------------------------------------- 4.5/241.3 MB 3.9 MB/s eta 0:01:00\n",
      "    --------------------------------------- 5.2/241.3 MB 3.9 MB/s eta 0:01:00\n",
      "    --------------------------------------- 5.5/241.3 MB 3.9 MB/s eta 0:01:00\n",
      "   - -------------------------------------- 6.6/241.3 MB 3.8 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 7.1/241.3 MB 3.7 MB/s eta 0:01:03\n",
      "   - -------------------------------------- 8.1/241.3 MB 3.9 MB/s eta 0:01:01\n",
      "   - -------------------------------------- 8.9/241.3 MB 3.8 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 9.4/241.3 MB 3.8 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 10.5/241.3 MB 3.8 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 11.0/241.3 MB 3.7 MB/s eta 0:01:02\n",
      "   - -------------------------------------- 11.8/241.3 MB 3.7 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 12.3/241.3 MB 3.7 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 13.1/241.3 MB 3.7 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 13.9/241.3 MB 3.7 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 14.4/241.3 MB 3.6 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 15.2/241.3 MB 3.6 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 16.0/241.3 MB 3.6 MB/s eta 0:01:03\n",
      "   -- ------------------------------------- 16.8/241.3 MB 3.6 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 17.6/241.3 MB 3.6 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 18.4/241.3 MB 3.6 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 18.9/241.3 MB 3.6 MB/s eta 0:01:02\n",
      "   --- ------------------------------------ 19.7/241.3 MB 3.6 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 20.4/241.3 MB 3.6 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 21.2/241.3 MB 3.6 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 22.0/241.3 MB 3.7 MB/s eta 0:01:01\n",
      "   --- ------------------------------------ 22.8/241.3 MB 3.7 MB/s eta 0:01:00\n",
      "   --- ------------------------------------ 23.6/241.3 MB 3.7 MB/s eta 0:01:00\n",
      "   ---- ----------------------------------- 24.4/241.3 MB 3.7 MB/s eta 0:01:00\n",
      "   ---- ----------------------------------- 25.2/241.3 MB 3.7 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 25.7/241.3 MB 3.7 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 26.5/241.3 MB 3.7 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 27.3/241.3 MB 3.7 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 28.0/241.3 MB 3.7 MB/s eta 0:00:59\n",
      "   ---- ----------------------------------- 28.8/241.3 MB 3.7 MB/s eta 0:00:58\n",
      "   ---- ----------------------------------- 29.6/241.3 MB 3.7 MB/s eta 0:00:58\n",
      "   ----- ---------------------------------- 30.4/241.3 MB 3.7 MB/s eta 0:00:58\n",
      "   ----- ---------------------------------- 31.2/241.3 MB 3.7 MB/s eta 0:00:58\n",
      "   ----- ---------------------------------- 32.0/241.3 MB 3.7 MB/s eta 0:00:58\n",
      "   ----- ---------------------------------- 32.5/241.3 MB 3.7 MB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 33.3/241.3 MB 3.7 MB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 34.1/241.3 MB 3.7 MB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 34.9/241.3 MB 3.7 MB/s eta 0:00:57\n",
      "   ----- ---------------------------------- 35.7/241.3 MB 3.7 MB/s eta 0:00:57\n",
      "   ------ --------------------------------- 36.4/241.3 MB 3.7 MB/s eta 0:00:56\n",
      "   ------ --------------------------------- 37.0/241.3 MB 3.7 MB/s eta 0:00:56\n",
      "   ------ --------------------------------- 37.7/241.3 MB 3.7 MB/s eta 0:00:56\n",
      "   ------ --------------------------------- 38.5/241.3 MB 3.7 MB/s eta 0:00:56\n",
      "   ------ --------------------------------- 39.3/241.3 MB 3.7 MB/s eta 0:00:56\n",
      "   ------ --------------------------------- 39.8/241.3 MB 3.7 MB/s eta 0:00:56\n",
      "   ------ --------------------------------- 40.6/241.3 MB 3.7 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 41.4/241.3 MB 3.7 MB/s eta 0:00:55\n",
      "   ------ --------------------------------- 42.2/241.3 MB 3.7 MB/s eta 0:00:55\n",
      "   ------- -------------------------------- 43.0/241.3 MB 3.7 MB/s eta 0:00:55\n",
      "   ------- -------------------------------- 43.5/241.3 MB 3.7 MB/s eta 0:00:55\n",
      "   ------- -------------------------------- 44.6/241.3 MB 3.7 MB/s eta 0:00:54\n",
      "   ------- -------------------------------- 45.1/241.3 MB 3.7 MB/s eta 0:00:54\n",
      "   ------- -------------------------------- 45.9/241.3 MB 3.7 MB/s eta 0:00:54\n",
      "   ------- -------------------------------- 46.7/241.3 MB 3.7 MB/s eta 0:00:54\n",
      "   ------- -------------------------------- 47.4/241.3 MB 3.7 MB/s eta 0:00:53\n",
      "   ------- -------------------------------- 48.2/241.3 MB 3.7 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 49.0/241.3 MB 3.7 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 49.5/241.3 MB 3.7 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 50.3/241.3 MB 3.7 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 51.1/241.3 MB 3.7 MB/s eta 0:00:53\n",
      "   -------- ------------------------------- 51.9/241.3 MB 3.7 MB/s eta 0:00:52\n",
      "   -------- ------------------------------- 52.7/241.3 MB 3.7 MB/s eta 0:00:52\n",
      "   -------- ------------------------------- 53.5/241.3 MB 3.7 MB/s eta 0:00:52\n",
      "   -------- ------------------------------- 54.0/241.3 MB 3.7 MB/s eta 0:00:52\n",
      "   --------- ------------------------------ 54.8/241.3 MB 3.7 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 55.6/241.3 MB 3.7 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 56.4/241.3 MB 3.7 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 56.9/241.3 MB 3.7 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 57.7/241.3 MB 3.7 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 58.5/241.3 MB 3.7 MB/s eta 0:00:51\n",
      "   --------- ------------------------------ 59.2/241.3 MB 3.7 MB/s eta 0:00:50\n",
      "   --------- ------------------------------ 60.0/241.3 MB 3.7 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 60.8/241.3 MB 3.7 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 61.6/241.3 MB 3.7 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 62.4/241.3 MB 3.7 MB/s eta 0:00:50\n",
      "   ---------- ----------------------------- 62.9/241.3 MB 3.7 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 63.7/241.3 MB 3.7 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 64.5/241.3 MB 3.7 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 65.3/241.3 MB 3.6 MB/s eta 0:00:49\n",
      "   ---------- ----------------------------- 66.1/241.3 MB 3.7 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 66.8/241.3 MB 3.6 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 67.4/241.3 MB 3.6 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 68.2/241.3 MB 3.6 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 68.9/241.3 MB 3.6 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 69.7/241.3 MB 3.6 MB/s eta 0:00:48\n",
      "   ----------- ---------------------------- 70.3/241.3 MB 3.6 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 71.0/241.3 MB 3.6 MB/s eta 0:00:47\n",
      "   ----------- ---------------------------- 71.8/241.3 MB 3.6 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 72.6/241.3 MB 3.6 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 73.4/241.3 MB 3.6 MB/s eta 0:00:47\n",
      "   ------------ --------------------------- 74.2/241.3 MB 3.6 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 74.7/241.3 MB 3.6 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 75.5/241.3 MB 3.6 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 76.3/241.3 MB 3.6 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 77.1/241.3 MB 3.6 MB/s eta 0:00:46\n",
      "   ------------ --------------------------- 77.9/241.3 MB 3.6 MB/s eta 0:00:45\n",
      "   ------------ --------------------------- 78.4/241.3 MB 3.6 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 79.2/241.3 MB 3.6 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 80.0/241.3 MB 3.6 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 80.7/241.3 MB 3.6 MB/s eta 0:00:45\n",
      "   ------------- -------------------------- 81.3/241.3 MB 3.6 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 82.1/241.3 MB 3.6 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 82.8/241.3 MB 3.6 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 83.6/241.3 MB 3.6 MB/s eta 0:00:44\n",
      "   ------------- -------------------------- 84.1/241.3 MB 3.6 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 84.9/241.3 MB 3.6 MB/s eta 0:00:44\n",
      "   -------------- ------------------------- 85.7/241.3 MB 3.6 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 86.5/241.3 MB 3.6 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 87.3/241.3 MB 3.6 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 87.8/241.3 MB 3.6 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 88.6/241.3 MB 3.6 MB/s eta 0:00:43\n",
      "   -------------- ------------------------- 89.4/241.3 MB 3.6 MB/s eta 0:00:42\n",
      "   -------------- ------------------------- 90.2/241.3 MB 3.6 MB/s eta 0:00:42\n",
      "   --------------- ------------------------ 91.0/241.3 MB 3.6 MB/s eta 0:00:42\n",
      "   --------------- ------------------------ 91.5/241.3 MB 3.6 MB/s eta 0:00:42\n",
      "   --------------- ------------------------ 92.3/241.3 MB 3.6 MB/s eta 0:00:42\n",
      "   --------------- ------------------------ 93.1/241.3 MB 3.6 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 93.8/241.3 MB 3.6 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 94.6/241.3 MB 3.6 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 95.2/241.3 MB 3.6 MB/s eta 0:00:41\n",
      "   --------------- ------------------------ 95.9/241.3 MB 3.6 MB/s eta 0:00:41\n",
      "   ---------------- ----------------------- 96.7/241.3 MB 3.6 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 97.5/241.3 MB 3.6 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 98.3/241.3 MB 3.6 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 98.8/241.3 MB 3.6 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 99.6/241.3 MB 3.6 MB/s eta 0:00:40\n",
      "   ---------------- ----------------------- 100.4/241.3 MB 3.6 MB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 101.2/241.3 MB 3.6 MB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 102.0/241.3 MB 3.6 MB/s eta 0:00:39\n",
      "   ---------------- ----------------------- 102.5/241.3 MB 3.6 MB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 103.3/241.3 MB 3.6 MB/s eta 0:00:39\n",
      "   ----------------- ---------------------- 104.1/241.3 MB 3.6 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 104.9/241.3 MB 3.6 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 105.4/241.3 MB 3.6 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 106.2/241.3 MB 3.6 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 107.0/241.3 MB 3.6 MB/s eta 0:00:38\n",
      "   ----------------- ---------------------- 107.7/241.3 MB 3.6 MB/s eta 0:00:37\n",
      "   ----------------- ---------------------- 108.5/241.3 MB 3.6 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 109.3/241.3 MB 3.6 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 109.8/241.3 MB 3.6 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 110.6/241.3 MB 3.6 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 111.4/241.3 MB 3.6 MB/s eta 0:00:36\n",
      "   ------------------ --------------------- 112.2/241.3 MB 3.6 MB/s eta 0:00:36\n",
      "   ------------------ --------------------- 113.0/241.3 MB 3.6 MB/s eta 0:00:36\n",
      "   ------------------ --------------------- 113.5/241.3 MB 3.6 MB/s eta 0:00:36\n",
      "   ------------------ --------------------- 114.3/241.3 MB 3.6 MB/s eta 0:00:36\n",
      "   ------------------- -------------------- 115.1/241.3 MB 3.6 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 115.9/241.3 MB 3.6 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 116.7/241.3 MB 3.6 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 117.4/241.3 MB 3.6 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 118.0/241.3 MB 3.6 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 118.8/241.3 MB 3.6 MB/s eta 0:00:34\n",
      "   ------------------- -------------------- 119.5/241.3 MB 3.6 MB/s eta 0:00:34\n",
      "   ------------------- -------------------- 120.3/241.3 MB 3.6 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 120.8/241.3 MB 3.6 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 121.6/241.3 MB 3.6 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 122.4/241.3 MB 3.6 MB/s eta 0:00:33\n",
      "   -------------------- ------------------- 123.2/241.3 MB 3.6 MB/s eta 0:00:33\n",
      "   -------------------- ------------------- 124.0/241.3 MB 3.6 MB/s eta 0:00:33\n",
      "   -------------------- ------------------- 124.8/241.3 MB 3.6 MB/s eta 0:00:33\n",
      "   -------------------- ------------------- 125.3/241.3 MB 3.6 MB/s eta 0:00:33\n",
      "   -------------------- ------------------- 125.8/241.3 MB 3.6 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 126.9/241.3 MB 3.6 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 127.7/241.3 MB 3.6 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 128.2/241.3 MB 3.6 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 129.0/241.3 MB 3.6 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 129.5/241.3 MB 3.6 MB/s eta 0:00:32\n",
      "   --------------------- ------------------ 130.3/241.3 MB 3.6 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 131.1/241.3 MB 3.6 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 131.9/241.3 MB 3.6 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 132.6/241.3 MB 3.6 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 133.2/241.3 MB 3.6 MB/s eta 0:00:31\n",
      "   ---------------------- ----------------- 134.0/241.3 MB 3.6 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 134.7/241.3 MB 3.6 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 135.5/241.3 MB 3.6 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 136.3/241.3 MB 3.6 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 136.8/241.3 MB 3.6 MB/s eta 0:00:30\n",
      "   ---------------------- ----------------- 137.6/241.3 MB 3.6 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 138.4/241.3 MB 3.6 MB/s eta 0:00:29\n",
      "   ----------------------- ---------------- 139.2/241.3 MB 3.6 MB/s eta 0:00:29\n",
      "   ----------------------- ---------------- 140.0/241.3 MB 3.6 MB/s eta 0:00:29\n",
      "   ----------------------- ---------------- 140.8/241.3 MB 3.6 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 141.3/241.3 MB 3.6 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 142.1/241.3 MB 3.6 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 142.9/241.3 MB 3.6 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 143.7/241.3 MB 3.6 MB/s eta 0:00:28\n",
      "   ----------------------- ---------------- 144.4/241.3 MB 3.6 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 145.0/241.3 MB 3.6 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 145.8/241.3 MB 3.6 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 146.5/241.3 MB 3.6 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 147.3/241.3 MB 3.6 MB/s eta 0:00:27\n",
      "   ------------------------ --------------- 148.1/241.3 MB 3.6 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 148.6/241.3 MB 3.6 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 149.4/241.3 MB 3.6 MB/s eta 0:00:26\n",
      "   ------------------------ --------------- 150.2/241.3 MB 3.6 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 151.0/241.3 MB 3.6 MB/s eta 0:00:26\n",
      "   ------------------------- -------------- 151.8/241.3 MB 3.6 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 152.3/241.3 MB 3.6 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 153.1/241.3 MB 3.6 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 153.9/241.3 MB 3.6 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 154.7/241.3 MB 3.6 MB/s eta 0:00:25\n",
      "   ------------------------- -------------- 155.5/241.3 MB 3.6 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 156.0/241.3 MB 3.6 MB/s eta 0:00:24\n",
      "   ------------------------- -------------- 156.8/241.3 MB 3.6 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 157.5/241.3 MB 3.6 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 158.3/241.3 MB 3.6 MB/s eta 0:00:24\n",
      "   -------------------------- ------------- 159.1/241.3 MB 3.6 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 159.9/241.3 MB 3.6 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 160.4/241.3 MB 3.6 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 161.2/241.3 MB 3.6 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 162.0/241.3 MB 3.6 MB/s eta 0:00:23\n",
      "   -------------------------- ------------- 162.8/241.3 MB 3.6 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 163.6/241.3 MB 3.6 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 164.1/241.3 MB 3.6 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 164.9/241.3 MB 3.6 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 165.7/241.3 MB 3.6 MB/s eta 0:00:22\n",
      "   --------------------------- ------------ 166.5/241.3 MB 3.6 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 167.2/241.3 MB 3.6 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 167.8/241.3 MB 3.6 MB/s eta 0:00:21\n",
      "   --------------------------- ------------ 168.6/241.3 MB 3.6 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 169.3/241.3 MB 3.6 MB/s eta 0:00:21\n",
      "   ---------------------------- ----------- 170.1/241.3 MB 3.6 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 170.9/241.3 MB 3.6 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 171.7/241.3 MB 3.6 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 172.2/241.3 MB 3.6 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 173.0/241.3 MB 3.6 MB/s eta 0:00:20\n",
      "   ---------------------------- ----------- 173.8/241.3 MB 3.6 MB/s eta 0:00:19\n",
      "   ---------------------------- ----------- 174.6/241.3 MB 3.6 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 175.4/241.3 MB 3.6 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 176.2/241.3 MB 3.6 MB/s eta 0:00:19\n",
      "   ----------------------------- ---------- 176.9/241.3 MB 3.6 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 177.7/241.3 MB 3.6 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 178.5/241.3 MB 3.6 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 179.3/241.3 MB 3.6 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 180.1/241.3 MB 3.6 MB/s eta 0:00:18\n",
      "   ----------------------------- ---------- 180.6/241.3 MB 3.6 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 181.4/241.3 MB 3.6 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 182.2/241.3 MB 3.6 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 183.0/241.3 MB 3.6 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 183.8/241.3 MB 3.6 MB/s eta 0:00:17\n",
      "   ------------------------------ --------- 184.3/241.3 MB 3.6 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 185.1/241.3 MB 3.6 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 185.9/241.3 MB 3.6 MB/s eta 0:00:16\n",
      "   ------------------------------ --------- 186.6/241.3 MB 3.6 MB/s eta 0:00:16\n",
      "   ------------------------------- -------- 187.4/241.3 MB 3.6 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 188.0/241.3 MB 3.6 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 188.7/241.3 MB 3.6 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 189.5/241.3 MB 3.6 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 190.3/241.3 MB 3.6 MB/s eta 0:00:15\n",
      "   ------------------------------- -------- 191.1/241.3 MB 3.6 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 191.6/241.3 MB 3.6 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 192.4/241.3 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 193.2/241.3 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 194.0/241.3 MB 3.6 MB/s eta 0:00:14\n",
      "   -------------------------------- ------- 194.8/241.3 MB 3.6 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 195.6/241.3 MB 3.6 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 196.1/241.3 MB 3.6 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 196.9/241.3 MB 3.6 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 197.7/241.3 MB 3.6 MB/s eta 0:00:13\n",
      "   -------------------------------- ------- 198.4/241.3 MB 3.6 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 199.2/241.3 MB 3.6 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 199.8/241.3 MB 3.6 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 200.5/241.3 MB 3.6 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 201.3/241.3 MB 3.6 MB/s eta 0:00:12\n",
      "   --------------------------------- ------ 202.1/241.3 MB 3.6 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 202.6/241.3 MB 3.6 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 203.4/241.3 MB 3.6 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 204.2/241.3 MB 3.6 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 205.0/241.3 MB 3.6 MB/s eta 0:00:11\n",
      "   ---------------------------------- ----- 205.8/241.3 MB 3.6 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 206.3/241.3 MB 3.6 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 207.1/241.3 MB 3.6 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 207.9/241.3 MB 3.6 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 208.7/241.3 MB 3.6 MB/s eta 0:00:10\n",
      "   ---------------------------------- ----- 209.5/241.3 MB 3.6 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 210.0/241.3 MB 3.6 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 210.8/241.3 MB 3.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 211.6/241.3 MB 3.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 212.3/241.3 MB 3.6 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 213.1/241.3 MB 3.6 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 213.9/241.3 MB 3.6 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 214.4/241.3 MB 3.6 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 215.2/241.3 MB 3.6 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 215.7/241.3 MB 3.6 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 216.5/241.3 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 217.3/241.3 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 218.1/241.3 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 218.9/241.3 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 219.7/241.3 MB 3.6 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 220.2/241.3 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 221.0/241.3 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 221.8/241.3 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 222.6/241.3 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 223.3/241.3 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 224.1/241.3 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 224.7/241.3 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 225.4/241.3 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 226.2/241.3 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 227.0/241.3 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 227.8/241.3 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 228.6/241.3 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 229.1/241.3 MB 3.6 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 229.9/241.3 MB 3.6 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 230.7/241.3 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 231.5/241.3 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 232.0/241.3 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 232.8/241.3 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 233.6/241.3 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 234.4/241.3 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 235.1/241.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  235.7/241.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  236.5/241.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  237.2/241.3 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  238.0/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  238.8/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  239.6/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  240.1/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  240.9/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  241.2/241.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 241.3/241.3 MB 3.5 MB/s  0:01:07\n",
      "Downloading torchvision-0.23.0-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.6 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 3.6 MB/s  0:00:00\n",
      "Downloading torchaudio-2.8.0-cp312-cp312-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.5 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 3.7 MB/s  0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 3.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.3/6.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.3 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.2/6.3 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 3.6 MB/s  0:00:01\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "\n",
      "  Attempting uninstall: sympy\n",
      "\n",
      "    Found existing installation: sympy 1.12\n",
      "\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "    Uninstalling sympy-1.12:\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "      Successfully uninstalled sympy-1.12\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------------------------------------- 0/4 [sympy]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ---------------------------------------- 4/4 [torchaudio]\n",
      "\n",
      "Successfully installed sympy-1.14.0 torch-2.8.0 torchaudio-2.8.0 torchvision-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c322a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PyTorch Implementation ===\n",
      "PyTorch data prepared successfully!\n",
      "Data batch shape: torch.Size([32, 64])\n",
      "Labels batch shape: torch.Size([32])\n",
      "MLP_PyTorch(\n",
      "  (layer1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (layer2): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Training PyTorch model...\n",
      "Epoch 1/10, Loss: 2.2285\n",
      "Epoch 2/10, Loss: 1.9813\n",
      "Epoch 3/10, Loss: 1.6428\n",
      "Epoch 4/10, Loss: 1.2868\n",
      "Epoch 5/10, Loss: 0.9939\n",
      "Epoch 6/10, Loss: 0.7833\n",
      "Epoch 7/10, Loss: 0.6378\n",
      "Epoch 8/10, Loss: 0.5361\n",
      "Epoch 9/10, Loss: 0.4607\n",
      "Epoch 10/10, Loss: 0.4051\n",
      "PyTorch training finished!\n",
      "PyTorch Model Accuracy: 91.67%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PART 6: PyTorch Implementation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== PyTorch Implementation ===\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import TensorDataset, DataLoader\n",
    "    \n",
    "    # TASK 6.1: Convert data to PyTorch Tensors and create DataLoaders\n",
    "    X_train_tensor = torch.from_numpy(X_train).float()\n",
    "    y_train_tensor = torch.from_numpy(y_train).long()\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(X_test).float()\n",
    "    y_test_tensor = torch.from_numpy(y_test).long()\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"PyTorch data prepared successfully!\")\n",
    "    \n",
    "    # Check a batch\n",
    "    data_batch, labels_batch = next(iter(train_loader))\n",
    "    print(f\"Data batch shape: {data_batch.shape}\")\n",
    "    print(f\"Labels batch shape: {labels_batch.shape}\")\n",
    "    \n",
    "    # TASK 6.2: Define the neural network architecture\n",
    "    class MLP_PyTorch(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layer1 = nn.Linear(64, 32)\n",
    "            self.activation = nn.ReLU()\n",
    "            self.layer2 = nn.Linear(32, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.layer2(x)\n",
    "            return x\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = MLP_PyTorch()\n",
    "    print(model)\n",
    "    \n",
    "    # TASK 6.3: Instantiate the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # TASK 6.4: Complete the PyTorch training loop\n",
    "    n_epochs = 10\n",
    "    \n",
    "    print(\"\\nTraining PyTorch model...\")\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # 1. Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. Forward pass\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            # 3. Calculate the loss\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # 4. Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"PyTorch training finished!\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'PyTorch Model Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"PyTorch not available - skipping PyTorch implementation\")\n",
    "    print(\"To install PyTorch, run: pip install torch torchvision\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
